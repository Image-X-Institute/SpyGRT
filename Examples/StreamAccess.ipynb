{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpyGRT StreamAccess Example\n",
    " **Author: Youssef Ben Bouchta <br>\n",
    " Last Edited: 17/03/2022** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# Example of how to initialise stream objects and basic stram control commands.\n",
    "# Author: Youssef Ben Bouchta\n",
    "# Last Edited: 29/03/2022\n",
    "\n",
    "#Imports\n",
    "import pyrealsense2 as rs2\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from spygrt.stream import Recording, Camera, NoMoreFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Initialising the two stream classes\n",
    " \n",
    " The first step to using SpyGRT is to initialise a stream object. Stream object can be either: \n",
    " \n",
    "   1. A **Camera** object or;\n",
    "   2. A **Recording** object\n",
    "          \n",
    "**Camera** objects are used to handle real-time streams, whereas **Recording** objects are used to handle pre-recording streams saved as a rosbag *.bag* file. \n",
    "\n",
    "The following code initialises:\n",
    "\n",
    "  - A **Recording** stream, `myRecording` by loading the stream from the file located at `\\Data\\example.bag`. \n",
    "  - A **Camera** stream, `myCamera` by finding a connected realsense camera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no maximum distance setting for camera model: Intel RealSense D415\n",
      "no minimum distance setting for camera model: Intel RealSense D415\n"
     ]
    }
   ],
   "source": [
    "# Initialisation of a Recording\n",
    "\n",
    "## Filename of the recording\n",
    "## Example filename: 'C:/Data/recording.bag'\n",
    "## NB: Backslashes( \\ ) do not work for file path on windows\n",
    "## NB: Complete filepath can be ommited for files located in the working directory\n",
    "## NB: Filepath relative to current working directory are also allowed.\n",
    "\n",
    "filename = '../SampleData/Cam1_Obj.bag'\n",
    "\n",
    "myRecording = Recording(filename)\n",
    "\n",
    "#Initialisation of a real-time stream, i.e: a Camera object\n",
    "\n",
    "ctx = rs2.context()\n",
    "## Obtaining the first camera in the list of all realsense devices.\n",
    "## NB: If multiple realsense devices are connected, the first will be random. \n",
    "myFirstRealSenseDevice = ctx.query_devices()[0]\n",
    "myCamera = Camera(myFirstRealSenseDevice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing information about the stream\n",
    "\n",
    "The following code accesses and print the basic parameters of `myRecording`. \n",
    "\n",
    "The same commands could easily be used for `myCamera` as all stream classed have almost the same interface. If you prefer to follow this example with a real-time stream you can replace: \n",
    "\n",
    "`myStream = myRecording`\n",
    "\n",
    "with:\n",
    "\n",
    "`myStream = myCamera`\n",
    "\n",
    "in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial Number: 123122061763\n",
      "Camera Model: Intel RealSense D415\n",
      "Depth Scale: 0.0010000000474974513\n",
      "Camera intrinsics: \n",
      "[[606.26550293   0.         315.01315308]\n",
      " [  0.         606.26550293 252.1219635 ]\n",
      " [  0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# This part of this example will work for all stream objects.\n",
    "\n",
    "# If you want to work with a real-time stream, replace myRecording with myCamera.\n",
    "myStream = myRecording \n",
    "\n",
    "# Accessing device informations\n",
    "\n",
    "# Print serial number of the camera that acquire(s/d) the stream\n",
    "print(\"Serial Number: \" + str(myStream.serial))\n",
    "\n",
    "# Print model of the camera that acquire(s/d) the stream\n",
    "print(\"Camera Model: \" + str(myStream.model))\n",
    "\n",
    "# Print the depth scale -> depth data * depth scale = depth in meters\n",
    "print(\"Depth Scale: \" + str(myStream.depthScale))\n",
    "\n",
    "# Print the depth camera pinhole camera intrinsics - Needed for transforming depth frames to point clouds.\n",
    "# NB: the intrinsics will be given as an open3D intrinsics object.\n",
    "intrinsics = myStream.get_o3d_intrinsics()\n",
    "print(\"Camera intrinsics: \\n\" + str(intrinsics.intrinsic_matrix))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the initial frame\n",
    "\n",
    "The following code shows how to obtain depth and color frames using `myStream` which can be either a recording or a real-time stream.\n",
    "\n",
    "All stream objects acquire a frame during initialisation and save the last acquired frame. This frame is stored in the frame parameter until we call for a new frame twice (the first call for a new frame after initialisation returns the first frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accessing frame acquired during initialisation.\n",
    "## The frame parameter can also be used to access the last acquired frame.\n",
    "old_frame = myStream.frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling for a new frame\n",
    "\n",
    "To call for a new frame, use the `get_frames()` method of stream objects.\n",
    "\n",
    "Notice the call to `get_frames()` is within a try - except block. This is because if a stream cannot return a frame for any reason (the recording has ended, or we've called the `end_stream()` method), `get_frames()` will raise a `NoMoreFrames` exception. \n",
    "\n",
    "As we won't be calling get_frames() again in this example, we ask the stream object to stop streaming. We will still be able to access the frame we acquired by accessing the `.frame` parameters but won't be able to receive new frames by calling the `get_frames()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire and access a new frame\n",
    "# NB: get_frames can raise an unhandled exception if it is not possible to acquire a new frame\n",
    "# This can occur if you've reached the end of a recorded stream or if the end_stream method has \n",
    "# been called.\n",
    "try:\n",
    "    new_frame = myStream.get_frames()\n",
    "except NoMoreFrames:\n",
    "    print(\"Encountered a NoMoreFrames error\")\n",
    "    pass\n",
    "\n",
    "# NB: The first frame of a stream can be accessed with the first call to get_frames() or with the frame parameter\n",
    "\n",
    "# Obtaining an open3D pointcloud with depth data in meters\n",
    "\n",
    "\n",
    "#End the streams\n",
    "\n",
    "myStream.end_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing frame data\n",
    "\n",
    "To access the raw data from the frame that was acquired in the previous block:\n",
    "\n",
    "1. We assign the depth frame to `depth_frame` and the color frame to `color_frame` by accessing the different elements of the tuple returned by `get_frames()` which returns a tuple with the depth frame in position 0 and the color frame in position 1. \n",
    "\n",
    "   Note that `depth_frame` and `color_frame` will both be open3D type images. \n",
    "<br>\n",
    "   \n",
    "2. To access pixel values, we can transform the color and depth image to open3D tensors. \n",
    "\n",
    "   Note: Open3D tensors can be manipulated similarly to numpy arrays. \n",
    "<br>\n",
    "\n",
    "3. We show how to convert open3D tensors to numpy in case conversion to numpy is needed (for example to if you want to use openCV or another library to manipulate the image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the depth frame and color frame separately\n",
    "\n",
    "depth_frame = new_frame[0]\n",
    "color_frame = new_frame[1]\n",
    "\n",
    "# Converting to Open3D tensors\n",
    "\n",
    "depth_tensor = depth_frame.as_tensor()\n",
    "color_tensor = color_frame.as_tensor()\n",
    "\n",
    "# Converting to numpy array\n",
    "\n",
    "depth_array = depth_tensor.numpy()\n",
    "color_array = color_tensor.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a Point Cloud from a frame\n",
    "\n",
    "We can use open3D to transform a frame into a pointcloud object. Here we will use the identity matrix as the extrinsic matrix, however, in most cases you would find the extrinsic matrix by performing a calibration as shown in the CalibrationExample file. \n",
    "\n",
    "To obtain a pointcloud from our color and depth frame we: \n",
    "\n",
    "1. Define an open3D RGBD image and set aligned to True since stream objects always return aligned depth and color frames. \n",
    "\n",
    "\n",
    "2. Create an open3D point cloud object with the RGBD image as an input. Notice that we use 1000/65535 for the scale parameter. That is because stream object return depth frames with depth value between 0 and 1. To retrieve the z16 information, the depth value has to be multiplied by 65535. To get the information in meter, we have to divide the z16 data by 1000. We also limit the maximum depth value to 2 meters to trim any outliers/background points.\n",
    "\n",
    "\n",
    "\n",
    "3. We access the raw point cloud data using the point parameter of the open3D point cloud object. \n",
    "\n",
    "\n",
    "4. We convert the position and color information to numpy array and concatenate them to obtain a single Nx6 array containing all of the point cloud information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrinsic matrix - replace with actual matrix if known\n",
    "# NB: Extrinsic matrix represent the transformation from lab to camera, calibrator objects give the transformation matrix from camera to lab,\n",
    "### these two matrices are inverses.\n",
    "ext = o3d.core.Tensor(np.identity(4),o3d.core.Dtype.Float32)\n",
    "\n",
    "### Create an RGB+Depth image\n",
    "rgbd = o3d.t.geometry.RGBDImage(color_frame, depth_frame, aligned = True)\n",
    "\n",
    "### Create point cloud with maximum depth of 2 meters\n",
    "pcd = o3d.t.geometry.PointCloud.create_from_rgbd_image(rgbd, \n",
    "                                                       o3d.core.Tensor(intrinsics.intrinsic_matrix,\n",
    "                                                                       dtype = o3d.core.Dtype.Float32),\n",
    "                                                       ext, 1000/65535, depth_max = 2)\n",
    "\n",
    "# Extracting points position information as an Open3D tensor\n",
    "\n",
    "pcd_xyz_tensor = pcd.point['positions']\n",
    "\n",
    "# Extracting points color information as an Open3D tensor\n",
    "\n",
    "pcd_RGB_tensor = pcd.point['colors']\n",
    "\n",
    "# converting both to numpy array\n",
    "\n",
    "pcd_xyz_numpy = pcd_xyz_tensor.numpy()\n",
    "pcd_RGB_numpy = pcd_RGB_tensor.numpy()\n",
    "\n",
    "#Concatenating into a single Nx6 numpy array (x,y,z,R,G,B)\n",
    "\n",
    "pcd_numpy = np.concatenate((pcd_xyz_numpy, pcd_RGB_numpy),1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
